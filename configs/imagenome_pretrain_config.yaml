experiment_name: 'gloria_pretrain'
phase: 'pretrain'
random_seed: 0

lightning:
    trainer:
        gpus: '0'
        max_epochs: 50
        distributed_backend: 'dp'
        gradient_clip_val: 0.25
        lr: 0.00005
        precision: 16
        reload_dataloaders_every_n_epochs: 1
        limit_train_batches: 0.2
        limit_val_batches: 0.05
        check_val_every_n_epoch: 1
#         resume_from_checkpoint: /scratch/mcinerney.de/gloria_outputs5/ckpt/gloria_pretrain_1.0/2021_12_21_02_24_20/last.ckpt # abnormal
#         resume_from_checkpoint: /scratch/mcinerney.de/gloria_outputs5/ckpt/gloria_pretrain_1.0/2021_12_21_11_05_13/last.ckpt # position
#         resume_from_checkpoint: /scratch/mcinerney.de/gloria_outputs5/ckpt/gloria_pretrain_1.0/2021_12_22_12_56_07/last.ckpt # position2
#         resume_from_checkpoint: /scratch/mcinerney.de/gloria_outputs5/ckpt/gloria_pretrain_1.0/2021_12_24_10_44_13/last.ckpt # transformer
#         resume_from_checkpoint: /scratch/mcinerney.de/gloria_outputs5/ckpt/gloria_pretrain_1.0/2021_12_31_22_02_48/last.ckpt # masked
#         resume_from_checkpoint: /scratch/mcinerney.de/gloria_outputs5/ckpt/gloria_pretrain_1.0/2021_12_31_11_12_34/last.ckpt # kl
#         resume_from_checkpoint: /scratch/mcinerney.de/gloria_outputs5/ckpt/gloria_pretrain_1.0/2022_01_07_00_37_57/last.ckpt # mixed_masked
    checkpoint_callback:
        monitor: 'val_loss'
        dirpath: '/scratch/mcinerney.de/gloria_outputs7/ckpt'
        save_last: true 
        mode: min
        save_top_k: 10
    early_stopping_callback:
        monitor: 'val_loss'
        min_delta: 0.00
        patience: 10
        verbose: false
        mode: 'min'
    logger:
        logger_type: 'WandbLogger'
        save_dir: '/scratch/mcinerney.de/gloria_outputs7/'
        project: 'gloria_v7'
    evaluate_localization:
        eval_attn_overlay_mode: 'upsample'
        plot_attn_overlay_mode: 'upsample'
        log_train_every: 100
#    weight_instances_by_localization:
#        weight_mode: 'no_attn_score'
#        temp: .2

model:
    gloria:
        local_loss_weight: 1.0
        global_loss_weight: 1.0
        temp1: 4.0
        temp2: 5.0
        temp3: 10.0
        no_attn_vec: false
#         no_attn_loss_weight: 1.0
#         attention_divergence_loss_weight: .1
#         sparse_attn_weight: 1.0
    vision:
        model_name: 'resnet_50'
        freeze_cnn: false
        pretrained: true
    text:
        bert_type: "emilyalsentzer/Bio_ClinicalBERT"
        last_n_layers: 4
        aggregate_method: 'sum'
        norm: false
        embedding_dim: 768
        freeze_bert: false
        agg_tokens: true
#     image_position_embeddings:
#         num: 19
#     image_transformer:
#         num_heads: 12
#         num_layers: 1

data:
    dataset: imagenome
    text: 
        word_num: 97
        captions_per_image: 5
        full_report: true
    image:
        imsize: 256
    split_slices: ''
    parallel: true
    get_physio_creds: false
    mimic_cxr_download_directory: '/scratch/mcinerney.de/mimic-cxr'
    imagenome_download_directory: '/scratch/mcinerney.de/imagenome'
    gold_test: true
    randomize_reports: false
    group_by: 'sentence'
#     randomize_objects_mode: 'random_sentences'
#     limit_to: 'abnormal'
    mask_mode: 'word'
    mask_prob: .3
    prob_of_masking: 1

transforms: 
    norm: 'half'
    random_crop:
        crop_size: 224

train:
    update_interval: 1000
    batch_size: 48
    num_workers: 8
    nvis: 8
    rand_vis: false 
    optimizer: 
        name: 'Adam'
        weight_decay: 1e-6
    scheduler: 
        name: 'plateau'
        monitor: 'val_loss'
        inerval: 'epoch'
        frequency: 1

base_output_dir: '/scratch/mcinerney.de/gloria_outputs7/output'
